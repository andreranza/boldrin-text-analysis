---
title: "*Michele Boldrin* YouTube Channel: a general survey."
date: "`r format(Sys.time(), '%d/%m/%y')`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, fig.align = "center")
```

```{r}
library(tidyverse)
library(tokenizers)
library(textrecipes)
library(tidytext)
library(stopwords)
library(httr)
library(jsonlite)
library(knitr)
```

## Motivation

The following analysis was carried out to yield some insights on my preferred YouTube channel: [*Michele Boldrin*](https://www.youtube.com/channel/UCMOiTfbUXxUFqJJtCQGHrrA). In addition, this small-scale project gave me the opportunity to experiment with the YouTube API, JSON data structure, regular expressions and text mining concepts. 

## Description

The following analysis used both quantitative data retrieved from the YouTube API and text data consisting in the set of auto-generated video transcripts of the over six-hundred videos existing on the channel which were downloaded by means of the command-line program [*youtube-dl*](http://ytdl-org.github.io/youtube-dl/). Unfortunately, not all the videos had a text transcript, hence I decided to consider only those for which one was available.

Data for each video of the channel obtained by querying the API are the following:

* video title
* video ID
* number of views
* number of likes
* number of dislikes
* number of comments
* upload date and time
* duration
* video description

A slice of the final data set is shown below.

```{r}
data <- read_csv("data/boldrin-videos.csv") %>% 
  select(-c(link, vid_age, vidTitle, description, like_ratio, dislike_ratio)) %>% 
  head()

kable(data, caption = "Data set resulting from the API query", digits = 3,
      format.args = list(big.mark = ".", decimal.mark = ",", scientific = FALSE),
      col.names = c("Video ID", "Views", "Likes", "Comments", "Dislikes", 
                    "Duration (Min.)", "Date"))
```

Alongside the data coming from the API, each video transcripts, once downloaded, appeared almost unreadable as we can see in the excerpt below. In addition, as presented in the book [*Text Mining with R*](https://www.tidytextmining.com), in order to perform any sort of text analysis, each transcript should be arranged in a *tidy text format* which is a data structure defined as a table with one token per row. Each token consists of unit of text, usually one or two words, that represents that analysis. 
they should be arranged in the ** that places one token row as presented in the book  by Julia Silge and David Robinson, which is required in order to perform any sort of text analysis.

```{r}
txt_files <- dir("./transcripts")

# Filter only the txt files
txt_files <- keep(txt_files, ~ str_detect(.x, ".txt$"))

setwd("./transcripts")
transcripts <- map(txt_files, ~ read_file(.x))
setwd("../")

sample_txt <- unlist(str_split(transcripts[[1]], pattern = "\n"))
print(sample_txt[1:23])
```

After some effort using regular expressions and the **purrr** package, before converting it to a tidy text format, the excerpt above becomes as follow:

```{r}
t_cleaned <- dir("./transcripts-cleaned")
setwd("./transcripts-cleaned/")
t_cleaned <- map(t_cleaned, ~ read_file(.x))
setwd("../")
sample_txt_cleaned <- unlist(str_split(t_cleaned[[1]], pattern = " ", n = 5))
print(sample_txt_cleaned[1:23])
```

